{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2T4zN58itdrR",
    "tags": []
   },
   "source": [
    "# Vertex AI Pipelines & Tests\n",
    "\n",
    "The scripts in this notebook have been updated in January 2024 by <chi-charles.zhang@lloydsbanking.com>\n",
    "\n",
    "\n",
    "References:   \n",
    "- Google offical docs: https://cloud.google.com/vertex-ai/docs/pipelines/introduction\n",
    "- A demo on YouTube: https://bit.ly/ml-pipeline-3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Environment & Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqB5XvP-U3Sn",
    "tags": []
   },
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5UmtuvhYixkz",
    "outputId": "6bf0e9ba-6604-4559-e090-bf6ca2e6e0d0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# updated to the latest dependencies on December 2023\n",
    "!pip install google-cloud-aiplatform==1.37.0 --upgrade --quiet\n",
    "!pip install google-cloud-pipeline-components==2.6.0 --upgrade --quiet\n",
    "!pip install kfp==2.4.0 --upgrade --quiet\n",
    "!pip install --upgrade --quiet pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PtTT_nZzevNK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiplatform version: 1.37.0\n",
      "google_cloud_pipeline_components version: 2.6.0\n",
      "KFP SDK version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "# check the installed packages\n",
    "!python3 -c \"from google.cloud import aiplatform; print('aiplatform version: {}'.format(aiplatform.__version__))\"\n",
    "!python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\"\n",
    "!python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1mcs7B5VUox_",
    "outputId": "402e6cc3-6990-4f73-8afc-27757b571e19",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "from kfp.dsl import pipeline\n",
    "from kfp.dsl import component\n",
    "from kfp.dsl import OutputPath\n",
    "from kfp.dsl import InputPath\n",
    "\n",
    "\n",
    "from kfp.dsl import Output\n",
    "from kfp.dsl import Metrics\n",
    "\n",
    "from kfp import compiler\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L21RLrfDFdcE",
    "tags": []
   },
   "source": [
    "### Constant variables\n",
    "Setting up the following constant variables:\n",
    "\n",
    "- project id \n",
    "- bucket storage\n",
    "- pipeline root folder\n",
    "- region\n",
    "- service account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  playpen-ed7014\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PROJECT_ID = \"\"\n",
    "# Get Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket URL:  gs://data-science-playpen-ed7014-bucket\n"
     ]
    }
   ],
   "source": [
    "# create a variable to store the bucket name\n",
    "BUCKET_NAME=\"gs://\" + \"data-science-\" + PROJECT_ID + \"-bucket\"\n",
    "print(\"Bucket URL: \", BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Root URL:  gs://data-science-playpen-ed7014-bucket/my_pipelines/\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/my_pipelines/\"\n",
    "print(\"Pipeline Root URL: \", PIPELINE_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = 'europe-west2'  # London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"playpen-ed7014-consumer-sa@playpen-ed7014.iam.gserviceaccount.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pD8XJufeSDAm"
   },
   "source": [
    "### Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_v5AkYzaFb3Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID,\n",
    "                location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCSPjj-D4Fao",
    "tags": []
   },
   "source": [
    "## Pipeline 1: A basic pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gyZkv8xyenN",
    "tags": []
   },
   "source": [
    "### Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onddQwd1U1qR",
    "outputId": "a3034a60-2c93-4e9b-f2aa-1bdd61f54246",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.10\")\n",
    "def combine_two_strings(string1: str, string2: str) -> str:\n",
    "    return string1 + \" \" + string2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ma7xWwLLVSlr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.10\")\n",
    "def swap_words(original_string: str)->NamedTuple(\"output\", [(\"before\", str), (\"after\", str)]):\n",
    "    words = original_string.split(\" \") \n",
    "    swapped_string = words[1] + \" \" + words[0]\n",
    "    return original_string, swapped_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZxMEEIWygAg"
   },
   "source": [
    "### Basic Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUrZade1VZGy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@pipeline(name=\"pipeline_1\",\n",
    "          pipeline_root=PIPELINE_ROOT + \"pipeline_1\")\n",
    "def pipeline_1(first_string: str='Hello', second_string: str='Pipeline'):\n",
    "    combine_task = combine_two_strings(string1=first_string, string2=second_string)\n",
    "    swap_task = swap_words(original_string=combine_task.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTL5tzhSXRpG"
   },
   "source": [
    "### Compile\n",
    "\n",
    "The compiler takes our pipeline (or component) function and compiles it into our pipeline specifiction as yaml file. This yaml file we can use to create our pipeline (or component) in Vertex AI Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iF0r0-CwVgpF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=combine_two_strings, \n",
    "    package_path=\"component_combine_two_strings.yaml\",\n",
    ")\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=swap_words, \n",
    "    package_path=\"component_swap_words.yaml\",\n",
    ")\n",
    "\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline_1, \n",
    "    package_path=\"pipeline_1.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# view component file\n",
    "# !cat ./component_combine_two_strings.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4_93MkF369u"
   },
   "source": [
    "### Run\n",
    "\n",
    "Create the run job using the API.\n",
    "You can also directly upload the pipeline JSON file in the Vertx AI UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwlXs0reY56x",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_job_1 = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"pipeline_1\",\n",
    "    template_path=\"pipeline_1.yaml\",\n",
    "    parameter_values={\"first_string\": \"Hello\", \"second_string\": \"Pipeline\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aogrQzmBZs7T",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_job_1.run(service_account=SERVICE_ACCOUNT,\n",
    "                   sync=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIqsDew7Yn1o",
    "tags": []
   },
   "source": [
    "## Pipeline 2: Retrieve and reuse a component\n",
    "In this example we reuse the component persistently stored in a yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.10\")\n",
    "def swap_words(original_string: str)->NamedTuple(\"output\", [(\"before\", str), (\"after\", str)]):\n",
    "    words = original_string.split(\" \") \n",
    "    swapped_string = words[1] + \" \" + words[0]\n",
    "    return original_string, swapped_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AW-PsUmPYuFo"
   },
   "outputs": [],
   "source": [
    "@pipeline(name=\"pipeline_2\",\n",
    "          pipeline_root=PIPELINE_ROOT + \"pipeline_2\")\n",
    "def reuse_component_pipeline(first_string: str='Reuse', second_string: str='Component'):\n",
    "    # retrieve the component\n",
    "    component_combine_two_strings = kfp.components.load_component_from_file('./component_combine_two_strings.yaml')\n",
    "    \n",
    "    combine_task = component_combine_two_strings(string1=first_string, string2=second_string)\n",
    "    swap_task = swap_words(original_string=combine_task.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hmW81e_WZ1mZ",
    "outputId": "2f25c05a-a0d3-447a-eb5f-aec7bba84233"
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=reuse_component_pipeline, \n",
    "    package_path=\"pipeline_2.yaml\"\n",
    ")\n",
    "\n",
    "pipeline_job_2 = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"pipeline_2\",\n",
    "    template_path=\"pipeline_2.yaml\",\n",
    "    parameter_values={\"first_string\": \"Reuse\", \"second_string\": \"Component\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_job_2.run(service_account=SERVICE_ACCOUNT,\n",
    "                   sync=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZhGKvIO0FKu",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Schedule a pipeline (using Pipeline 1 as example)\n",
    "\n",
    "We can create a pipeline run schedule in the following ways:\n",
    "1. Creating a schedule using the `PipelineJobSchedule.create` method.\n",
    "2. Create a schedule based on a PipelineJob using the `PipelineJob.create_schedule` method.\n",
    "\n",
    "The following example demonstrates the former way, `PipelineJobSchedule.create` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onddQwd1U1qR",
    "outputId": "a3034a60-2c93-4e9b-f2aa-1bdd61f54246",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Define components and pipeline ===\n",
    "\n",
    "@component(base_image=\"python:3.10\")\n",
    "def combine_two_strings(string1: str, string2: str) -> str:\n",
    "    return string1 + \" \" + string2\n",
    "\n",
    "@component(base_image=\"python:3.10\")\n",
    "def swap_words(original_string: str)->NamedTuple(\"output\", [(\"before\", str), (\"after\", str)]):\n",
    "    words = original_string.split(\" \") \n",
    "    swapped_string = words[1] + \" \" + words[0]\n",
    "    return original_string, swapped_string\n",
    "\n",
    "@pipeline(name=\"pipeline_1_schedule\",\n",
    "          pipeline_root=PIPELINE_ROOT + \"pipeline_1_schedule\")\n",
    "def schedule_pipeline_1(first_string: str='Hello', second_string: str='Schedule'):\n",
    "    combine_task = combine_two_strings(string1=first_string, string2=second_string)\n",
    "    swap_task = swap_words(original_string=combine_task.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=schedule_pipeline_1, \n",
    "    package_path=\"pipeline_1_schedule.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "pipeline_job_1_schedule = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"pipeline_1_schedule\",\n",
    "    template_path=\"pipeline_1_schedule.yaml\",\n",
    "    parameter_values={\"first_string\": \"Hello\", \"second_string\": \"Pipeline\"}\n",
    ")\n",
    "\n",
    "pipeline_job_schedule = aiplatform.PipelineJobSchedule(\n",
    "    pipeline_job=pipeline_job_1_schedule,\n",
    "    display_name=\"pipeline_1_schedule\"\n",
    ")\n",
    "\n",
    "pipeline_job_schedule.create(\n",
    "    cron=\"*/1 * * * *\",   # runs every 1 minute; also see cron definition, https://en.wikipedia.org/wiki/Cron\n",
    "    max_concurrent_run_count=3,  # The maximum number of concurrent runs for the schedule.\n",
    "    max_run_count=5,  # The maximum number of pipeline runs that the schedule creates after which it's completed.\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPKvcwXX4KDe",
    "tags": []
   },
   "source": [
    "## Pipeline 3: using the TF and GPU based image (\"tf2-gpu.2-6\") in a component\n",
    "\n",
    "- The following is an example how you can add an TF framework with GPU to your component.     \n",
    "- For example the training componentn that needs access to accelerators.\n",
    "- This job takes about 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3v231zyV4MUO"
   },
   "outputs": [],
   "source": [
    "@component(base_image=\"gcr.io/deeplearning-platform-release/tf2-gpu.2-6\")\n",
    "def tf_gpu_training_image() -> bool:\n",
    "    import logging\n",
    "    import tensorflow as tf\n",
    "\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "        logging.info('Name: {} Type: {} TF_version: {}'.format(gpu.name, gpu.device_type, tf.version.VERSION))\n",
    "    print(\"TersonFlow version:\", tf.version.VERSION)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sN_qYZE-6Nfn"
   },
   "outputs": [],
   "source": [
    "@pipeline(name=\"pipeline_3\",\n",
    "          pipeline_root=PIPELINE_ROOT + \"pipeline_3\")\n",
    "def tf_gpu_pipeline():\n",
    "    gpu_training = tf_gpu_training_image()\n",
    "    gpu_training.add_node_selector_constraint(accelerator=\"NVIDIA_TESLA_T4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZRXVqzdi7rdD",
    "outputId": "e02bc6fc-68f5-47f9-92e7-da07ef6f8393"
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=tf_gpu_training_image, \n",
    "    package_path=\"tf_gpu_training_image.yaml\"\n",
    ")\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=tf_gpu_pipeline, \n",
    "    package_path=\"pipeline_3.yaml\"\n",
    ")\n",
    "\n",
    "pipeline_job_3 = pipeline_jobs.PipelineJob(\n",
    "   display_name=\"pipeline_3\",\n",
    "   template_path=\"pipeline_3.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_job_3.run(service_account=SERVICE_ACCOUNT,\n",
    "                   sync=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vx9r4CJg2xkY",
    "tags": []
   },
   "source": [
    "## Pipeline 4: Adding additional dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "al_sL-1f2zYK"
   },
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.10\",\n",
    "           packages_to_install = [\"pandas==2.1.4\"],)\n",
    "def additional_packages():\n",
    "    import pandas\n",
    "    print(\"Pandas version: \", pandas.__version__)\n",
    "\n",
    "# !We expect pipeline fails for demonstration purposes\n",
    "@component(base_image=\"python:3.10\")\n",
    "def additional_packages_missing():\n",
    "    import pandas\n",
    "    print(\"Pandas version: \", pandas.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jJl27Itt3iCw",
    "outputId": "2707b7e5-bdc4-4533-f23f-cdd0242ca61c"
   },
   "outputs": [],
   "source": [
    "@pipeline(name=\"pipeline_4\",\n",
    "          pipeline_root=PIPELINE_ROOT + \"pipeline_4\")\n",
    "def pipeline_4():\n",
    "    additional_packages_task = additional_packages()\n",
    "    additional_packages_missing_task = additional_packages_missing()\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline_4, \n",
    "    package_path=\"pipeline_4.yaml\"\n",
    ")\n",
    "\n",
    "pipeline_job_4 = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"pipeline_4\",\n",
    "    template_path=\"pipeline_4.yaml\"\n",
    ")\n",
    "\n",
    "pipeline_job_4.run(service_account=SERVICE_ACCOUNT,\n",
    "                   sync=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckZCJavxBtf7",
    "tags": []
   },
   "source": [
    "## Pipeline 5: End-to-end XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCymNXQuW15K"
   },
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CLJqZSzYBu2y"
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (Artifact,\n",
    "                     Dataset,\n",
    "                     Input,\n",
    "                     Model,\n",
    "                     Output,\n",
    "                     Metrics,\n",
    "                     ClassificationMetrics,\n",
    "                     component,\n",
    "                     Markdown)\n",
    "\n",
    "from kfp import compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6O8akfmUoWc"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "sPKVVep2BxU9"
   },
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.10\",\n",
    "           packages_to_install = [\n",
    "                                   \"pandas==2.1.4\",  # 1.3.4\n",
    "                                   \"scikit-learn==1.3.2\"  #1.0.1\n",
    "                                 ],\n",
    ")\n",
    "def get_data(\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_test: Output[Dataset],\n",
    "):\n",
    "\n",
    "    from sklearn import datasets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pandas as pd\n",
    "\n",
    "    # dataset https://www.kaggle.com/uciml/breast-cancer-wisconsin-data\n",
    "    data_raw = datasets.load_breast_cancer()\n",
    "    data = pd.DataFrame(data_raw.data, columns=data_raw.feature_names)\n",
    "    data[\"target\"] = data_raw.target\n",
    "\n",
    "    train, test = train_test_split(data, test_size=0.3)\n",
    "\n",
    "    train.to_csv(dataset_train.path)\n",
    "    test.to_csv(dataset_test.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmllUUq0UqDo"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4aVHg2lSByxy"
   },
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.10\",\n",
    "           packages_to_install = [\n",
    "                                   \"pandas==2.1.4\",  # 1.3.4\n",
    "                                   \"scikit-learn==1.3.2\", #xgboost requires scikitlearn #1.0.1\n",
    "                                   \"xgboost==2.0.3\",  # 1.5.1\n",
    "                                 ],\n",
    ")\n",
    "def train_model(\n",
    "    dataset: Input[Dataset],\n",
    "    model_artifact: Output[Model]\n",
    "):\n",
    "\n",
    "    from xgboost import XGBClassifier\n",
    "    import pandas as pd\n",
    "\n",
    "    data = pd.read_csv(dataset.path)\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        objective=\"binary:logistic\"\n",
    "    )\n",
    "    model.fit(\n",
    "        data.drop(columns=[\"target\"]),\n",
    "        data.target,\n",
    "    )\n",
    "\n",
    "    score = model.score(\n",
    "        data.drop(columns=[\"target\"]),\n",
    "        data.target,\n",
    "    )\n",
    "\n",
    "    model_artifact.metadata[\"train_score\"] = float(score)\n",
    "    model_artifact.metadata[\"framework\"] = \"XGBoost\"\n",
    "\n",
    "    print(model_artifact.path)\n",
    "\n",
    "    model.save_model(model_artifact.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkazD35dv0Un"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "k1AiObLIB1cx"
   },
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.10\",\n",
    "           packages_to_install = [\n",
    "                                   \"pandas==2.1.4\",  # 1.3.4\n",
    "                                   \"scikit-learn==1.3.2\", #xgboost requires scikitlearn #1.0.1\n",
    "                                   \"xgboost==2.0.3\",  # 1.5.1\n",
    "                                 ],\n",
    ")\n",
    "def eval_model(\n",
    "    test_set: Input[Dataset],\n",
    "    xgb_model: Input[Model],\n",
    "    metrics: Output[ClassificationMetrics],\n",
    "    # smetrics: Output[Metrics]\n",
    ") -> NamedTuple(\"Outputs\", [(\"deploy\", str)]):\n",
    "    \n",
    "    from xgboost import XGBClassifier\n",
    "    import pandas as pd\n",
    "\n",
    "    data = pd.read_csv(test_set.path)\n",
    "    model = XGBClassifier()\n",
    "    model.load_model(xgb_model.path)\n",
    "\n",
    "    score = model.score(\n",
    "        data.drop(columns=[\"target\"]),\n",
    "        data.target,\n",
    "    )\n",
    "\n",
    "    from sklearn.metrics import roc_curve\n",
    "    y_scores =  model.predict_proba(data.drop(columns=[\"target\"]))[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(\n",
    "         y_true=data.target.to_numpy(), y_score=y_scores, pos_label=True\n",
    "    )\n",
    "    # metrics.log_roc_curve(fpr.tolist(), tpr.tolist(), thresholds.tolist())\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    y_pred = model.predict(data.drop(columns=[\"target\"]))\n",
    "\n",
    "    metrics.log_confusion_matrix(\n",
    "       [\"False\", \"True\"],\n",
    "       confusion_matrix(\n",
    "           data.target, y_pred\n",
    "       ).tolist()\n",
    "    )\n",
    "\n",
    "#     xgb_model.metadata[\"test_score\"] = float(score)\n",
    "    # smetrics.log_metric(\"score\", float(score))\n",
    "\n",
    "\n",
    "    deploy = \"true\"\n",
    "    #compare threshold or to previous\n",
    "\n",
    "    return (deploy,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4qRVZ9Jv2NV"
   },
   "source": [
    "### Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "dZq-EaJngwFW"
   },
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.10\",\n",
    "           packages_to_install=[\"google-cloud-aiplatform==1.37.0\"])   # 1.3.0\n",
    "def deploy(\n",
    "    model: Input[Model],\n",
    "    project: str,\n",
    "    region: str,):\n",
    "\n",
    "    import logging\n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project=project, location=region)\n",
    "\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    logging.debug(model)\n",
    "\n",
    "    print(\"model: \", model)\n",
    "    print(\"model.uri: \", model.uri)\n",
    "\n",
    "    import os\n",
    "    path,file = os.path.split(model.uri)\n",
    "\n",
    "    import datetime\n",
    "\n",
    "    # datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    # serving image https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#xgboost\n",
    "    deployed_model = aiplatform.Model.upload(\n",
    "          display_name=\"xgboost-pipeline\",\n",
    "          artifact_uri = path,\n",
    "          serving_container_image_uri=\"europe-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-7:latest\",  # google pre-built-containers#xgboost\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRgW9szKv46I"
   },
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "z8JS0BA0B7za"
   },
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT + \"pipeline_5\",\n",
    "    # A name for the pipeline. Use to determine the pipeline Context.\n",
    "    name=\"pipeline_5\",\n",
    ")\n",
    "def pipeline_5():\n",
    "    dataset_op = get_data()\n",
    "    training_op = train_model(dataset=dataset_op.outputs[\"dataset_train\"])\n",
    "    eval_op = eval_model(\n",
    "        test_set=dataset_op.outputs[\"dataset_test\"],\n",
    "        xgb_model=training_op.outputs[\"model_artifact\"]\n",
    "    )\n",
    "\n",
    "    with dsl.If(\n",
    "        eval_op.outputs[\"deploy\"] == \"true\",\n",
    "        name=\"deploy\",\n",
    "    ):\n",
    "        deploy_op = deploy(\n",
    "                            model=training_op.outputs[\"model_artifact\"],\n",
    "                            project=PROJECT_ID,\n",
    "                            region=REGION,\n",
    "                          )\n",
    "\n",
    "    # we need a solution for xgb models\n",
    "    # its here https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api#aiplatform_deploy_model_custom_trained_model_sample-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15juxBrzgiLk"
   },
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sfz5joUmghwp",
    "outputId": "708f3444-2301-4fc4-8781-915f5fd9c8fd"
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline_5,\n",
    "    package_path='pipeline_5.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSDY3Rrh8OTq"
   },
   "source": [
    "### Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0g0qJwcG8QHK",
    "outputId": "aaefdb7d-0962-49f0-d7f5-270794e1dce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/209587640097/locations/europe-west2/pipelineJobs/pipeline-5-20240117112703\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/209587640097/locations/europe-west2/pipelineJobs/pipeline-5-20240117112703')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west2/pipelines/runs/pipeline-5-20240117112703?project=209587640097\n",
      "PipelineJob projects/209587640097/locations/europe-west2/pipelineJobs/pipeline-5-20240117112703 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/209587640097/locations/europe-west2/pipelineJobs/pipeline-5-20240117112703 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/209587640097/locations/europe-west2/pipelineJobs/pipeline-5-20240117112703 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/209587640097/locations/europe-west2/pipelineJobs/pipeline-5-20240117112703 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/209587640097/locations/europe-west2/pipelineJobs/pipeline-5-20240117112703 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "pipeline_job_5 = pipeline_jobs.PipelineJob(\n",
    "        display_name=\"pipeline_5\",\n",
    "        template_path=\"pipeline_5.yaml\"\n",
    ")\n",
    "\n",
    "pipeline_job_5.run(service_account=SERVICE_ACCOUNT,\n",
    "                   sync=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQgfRlEoMdP4",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Pipeline end to end (Huggingface, Sentiment)\n",
    "Work in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfu406NnMoP0"
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9UUEt5Y7NBqU"
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "from kfp.v2.dsl import pipeline\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.dsl import OutputPath\n",
    "from kfp.v2.dsl import InputPath\n",
    "\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component,\n",
    "                        Markdown)\n",
    "\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EacI7gQ6NDWQ"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"sascha-playground-doit\"\n",
    "PIPELINE_ROOT = \"gs://doit-vertex-demo/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCrjJKg6NEap"
   },
   "outputs": [],
   "source": [
    "# use this instead\n",
    "aiplatform.init(project=PROJECT_ID,\n",
    "                location='us-central1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByOiC-yKMp7N"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMlUTgh3NFy7"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = [\n",
    "        \"transformers==4.1.1\",\n",
    "        \"google-cloud-storage==1.35.0\",\n",
    "        \"scikit-learn==0.24.0\",\n",
    "        \"pandas==1.1.5\"\n",
    "    ],\n",
    "    base_image=\"gcr.io/deeplearning-platform-release/tf2-gpu.2-6\"\n",
    ")\n",
    "def train_model(\n",
    "    epochs: int,\n",
    "    model_artifact: Output[Model],\n",
    "    smetrics: Output[Metrics]\n",
    "):\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from transformers import DistilBertTokenizerFast\n",
    "    from transformers import TFDistilBertForSequenceClassification\n",
    "    from google.cloud import storage\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from io import StringIO\n",
    "\n",
    "    print('load distilbert')\n",
    "    # load model and tokenizer\n",
    "    model = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\")\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "    print('start training')\n",
    "    file = tf.io.gfile.GFile(\n",
    "        'gs://machine-learning-samples/datasets/sentiment/imdb/csv/dataset.csv', mode='r').read()\n",
    "    df = pd.read_csv(StringIO(file))\n",
    "\n",
    "    #df = df.head()\n",
    "\n",
    "    sentiments = df['sentiment'].values.tolist()\n",
    "    reviews = df['review'].values.tolist()\n",
    "\n",
    "    training_sentences, validation_sentences, training_labels, validation_labels = train_test_split(\n",
    "        reviews,\n",
    "        sentiments,\n",
    "        test_size=.2)\n",
    "\n",
    "    train_encodings = tokenizer(training_sentences,\n",
    "                                truncation=True,\n",
    "                                padding=True)\n",
    "    val_encodings = tokenizer(validation_sentences,\n",
    "                              truncation=True,\n",
    "                              padding=True)\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(train_encodings),\n",
    "        training_labels\n",
    "    ))\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(val_encodings),\n",
    "        validation_labels\n",
    "    ))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=model.compute_loss,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_dataset.shuffle(100).batch(16),\n",
    "              epochs=epochs,\n",
    "              batch_size=16,)\n",
    "\n",
    "    model.save_pretrained(model_artifact.path)\n",
    "    print(model_artifact.path)\n",
    "\n",
    "    print('eval')\n",
    "    evaluation = model.evaluate(val_dataset.shuffle(100).batch(16),\n",
    "               batch_size=16)\n",
    "    print(evaluation)\n",
    "\n",
    "    smetrics.log_metric(\"accuracy\", float(evaluation[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCY9KVu7MrDC"
   },
   "source": [
    "## Serving Container\n",
    "code for the container see https://github.com/SaschaHeyer/serving-custom-container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8J-Fbxo1NHYX"
   },
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\n",
    "    \"google-cloud-build==3.8.3\",\n",
    "    \"google-api-python-client\"])\n",
    "def build_serving_container(model_artifact: Input[Model]) -> NamedTuple(\"Outputs\", [(\"container\", str)]):\n",
    "    from google.cloud.devtools import cloudbuild\n",
    "    from googleapiclient.discovery import build\n",
    "    import time\n",
    "\n",
    "    print('deploy.............')\n",
    "    print(model_artifact.uri)\n",
    "\n",
    "    client = cloudbuild.CloudBuildClient()\n",
    "    build = cloudbuild.Build()\n",
    "\n",
    "\n",
    "    # version is current timestamp\n",
    "    version = str(int(time.time()))\n",
    "    container = \"gcr.io/sascha-playground-doit/sentiment-fast-api-test:{}\".format(version)\n",
    "\n",
    "\n",
    "    #todo get the model from the pipeline folder\n",
    "    build.steps = [{\"name\": \"gcr.io/cloud-builders/git\",\n",
    "                    \"args\": [\"clone\", \"https://github.com/SaschaHeyer/serving-custom-container\"]},\n",
    "                   {\"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "                    \"args\": [\"cp\", \"-r\", \"gs://doit-vertex-demo/models/sentiment\", \"./serving-custom-container\"]},\n",
    "                   {\"name\": \"gcr.io/cloud-builders/docker\",\n",
    "                    \"args\": [\"build\", \"-t\", container, \"serving-custom-container\" ]},\n",
    "                   {\"name\": \"gcr.io/cloud-builders/docker\",\n",
    "                    \"args\": [\"push\", container]}]\n",
    "\n",
    "    #build.substitutions = {\"_VERSION\": version}\n",
    "\n",
    "    operation = client.create_build(project_id=\"sascha-playground-doit\", build=build)\n",
    "    # Print the in-progress operation\n",
    "    print(\"IN PROGRESS:\")\n",
    "    print(operation.metadata)\n",
    "\n",
    "    result = operation.result()\n",
    "    # Print the completed status\n",
    "    print(\"RESULT:\", result.status)\n",
    "\n",
    "    return (container,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXG_ErEiMsyX"
   },
   "source": [
    "## Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJtFKUurNQGJ"
   },
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-aiplatform==1.15.0\"])\n",
    "def deploy_model(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    container: str\n",
    "):\n",
    "  from google.cloud import aiplatform\n",
    "  aiplatform.init(project=project, location=region)\n",
    "\n",
    "  ENDPOINT_NAME = \"sentiment\"\n",
    "  DISPLAY_NAME  = \"sentiment\"\n",
    "\n",
    "  MODEL_TYPE = \"query\"\n",
    "  MODEL_NAME = f\"{MODEL_TYPE}_model\"  # Used by the deployment container.\n",
    "\n",
    "  def create_endpoint():\n",
    "    print('create endpoint')\n",
    "    endpoints = aiplatform.Endpoint.list(\n",
    "      filter='display_name=\"{}\"'.format(ENDPOINT_NAME),\n",
    "      order_by='create_time desc',\n",
    "      project=project,\n",
    "      location=region,\n",
    "    )\n",
    "\n",
    "    if len(endpoints) > 0:\n",
    "      endpoint = endpoints[0]\n",
    "    else:\n",
    "      endpoint = aiplatform.Endpoint.create(\n",
    "        display_name=ENDPOINT_NAME, project=project, location=region\n",
    "      )\n",
    "\n",
    "    return endpoint\n",
    "\n",
    "  models = aiplatform.Model.list(filter=(\"display_name={}\").format(DISPLAY_NAME))\n",
    "\n",
    "  PORT = 80\n",
    "  HEALTH_ROUTE = \"/health\"\n",
    "  PREDICT_ROUTE = \"/predict\"\n",
    "\n",
    "\n",
    "  if len(models) == 0:\n",
    "    # upload the initial model\n",
    "    model_uploaded = aiplatform.Model.upload(\n",
    "          display_name = DISPLAY_NAME,\n",
    "          serving_container_image_uri = container,\n",
    "          serving_container_health_route=HEALTH_ROUTE,\n",
    "          serving_container_predict_route=PREDICT_ROUTE,\n",
    "          serving_container_ports=[PORT]\n",
    "    )\n",
    "  else:\n",
    "    #upload a new model version using the exiting model ressource ID\n",
    "    parent_model = models[0].resource_name\n",
    "\n",
    "    model_uploaded = aiplatform.Model.upload(\n",
    "          parent_model = parent_model,\n",
    "          display_name = DISPLAY_NAME,\n",
    "          serving_container_image_uri = container,\n",
    "          serving_container_health_route=HEALTH_ROUTE,\n",
    "          serving_container_predict_route=PREDICT_ROUTE,\n",
    "          serving_container_ports=[PORT]\n",
    "    )\n",
    "\n",
    "  endpoint = create_endpoint()\n",
    "\n",
    "  model_deploy = model_uploaded.deploy(\n",
    "        machine_type=\"n1-standard-4\",\n",
    "        endpoint=endpoint,\n",
    "        traffic_split={\"0\": 100},\n",
    "        deployed_model_display_name=DISPLAY_NAME,\n",
    "  )\n",
    "\n",
    "  # undeploy models without traffic for this specific endpoint\n",
    "  for model in endpoint.list_models():\n",
    "    print(model)\n",
    "    if model.id not in endpoint.traffic_split:\n",
    "      endpoint.undeploy(deployed_model_id = model.id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB20e-knMt3m"
   },
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0dCWa5tNRzg"
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT + \"sentiment-pipeline\",\n",
    "    # A name for the pipeline. Use to determine the pipeline Context.\n",
    "    name=\"sentiment-pipeline\",\n",
    ")\n",
    "def pipeline(epochs:int):\n",
    "    train_op = train_model(epochs).add_node_selector_constraint(\n",
    "        label_name=\"cloud.google.com/gke-accelerator\",\n",
    "        value=\"NVIDIA_TESLA_T4\").set_caching_options(False)\n",
    "\n",
    "    # build custom serving container with latest model\n",
    "    build_serving_container_op = build_serving_container(train_op.outputs[\"model_artifact\"]).set_caching_options(False)\n",
    "\n",
    "    # upload and deploy model to vertex ai\n",
    "    deploy_op = deploy_model(\"sascha-playground-doit\",\"us-central1\", build_serving_container_op.outputs[\"container\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxDWoAwNMxqe"
   },
   "source": [
    "## Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ErZjGz6CNTWs",
    "outputId": "b310c2d6-8225-4969-a078-e84b4052bf9f"
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path='sentiment_pipeline.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e80cvl24Myrl"
   },
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZrat10WNUyd",
    "outputId": "197e58d5-8123-4dac-ba7a-dc2af81b00e2"
   },
   "outputs": [],
   "source": [
    "job = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"sentiment-pipeline\",\n",
    "    template_path=\"sentiment_pipeline.json\",\n",
    "    parameter_values={\n",
    "        'epochs': 3\n",
    "    }\n",
    ")\n",
    "\n",
    "job.submit(experiment=\"sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hA2zOlhZYBNm",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# What about artifacts that are not a dataset or model?\n",
    "\n",
    "For that we can use outputPath which provides similar like the artifacts a path to Google Cloud Storage where we can store data. In any case you need to serialize any intermediate object from memory to save it as a file. In your next component you then can load the serialized file back to in memory object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1KWS9ZQyYL00"
   },
   "outputs": [],
   "source": [
    "@component()\n",
    "def first(output_path: OutputPath()):\n",
    "\n",
    "  # everything that can be serialized to a file can be stored\n",
    "  # you are not limited to the artifact types like dataset or model\n",
    "\n",
    "  # common cases are tfidf\n",
    "\n",
    "  animals = ['cat', 'dog']\n",
    "\n",
    "  import pickle\n",
    "\n",
    "  with open(output_path + \"animals.pkl\", 'wb') as file:\n",
    "    pickle.dump(animals, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1EKc_9rYWHw"
   },
   "outputs": [],
   "source": [
    "@component()\n",
    "def second(input_path: InputPath()):\n",
    "\n",
    "  import pickle\n",
    "  file = open(input_path + \"animals.pkl\", 'rb')\n",
    "  data = pickle.load(file)\n",
    "  file.close()\n",
    "\n",
    "  print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ApQ80C7IaeiX"
   },
   "outputs": [],
   "source": [
    "@pipeline(name=\"output-path-pipeline\",\n",
    "          pipeline_root=PIPELINE_ROOT + \"output-path-pipeline\")\n",
    "def output_pipeline():\n",
    "    first_task = first()\n",
    "    second_task = second(first_task.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KtBerMSaa1UG",
    "outputId": "2988a385-3811-409b-e4b0-adfb5e7684f2"
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "  pipeline_func=output_pipeline, package_path=\"output_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7SmXK6aAbWdc"
   },
   "outputs": [],
   "source": [
    "job = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"output-pipeline\",\n",
    "    template_path=\"output_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_aWXapX6btqp",
    "outputId": "89893fc8-4f33-4c4d-82e7-d90f9c7051fa"
   },
   "outputs": [],
   "source": [
    "job.run(sync=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31_R3lA-bxNq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JGm1YhnUExE",
    "tags": []
   },
   "source": [
    "## Predefined Components\n",
    "\n",
    "For a full list of pre-defined components see https://cloud.google.com/vertex-ai/docs/pipelines/gcpc-list\n",
    "\n",
    "Predefined components depends on the use case.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "veJ5zQOxSqPJ",
    "YIqsDew7Yn1o",
    "IZhGKvIO0FKu",
    "Vx9r4CJg2xkY",
    "7JGm1YhnUExE",
    "UJL5drQCU3TO"
   ],
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-13.m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-13:m114"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
